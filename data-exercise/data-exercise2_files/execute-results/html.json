{
  "hash": "a0a4d7b50b63ec6496e073f73d5fe100",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data Exercise\"\nsubtitle: \"Analyzing text from public comments for REG 2023-02, Artificial Intelligence in Campaign Ads\"\n---\n\n\n# Introduction:\n\nThe source of the text for this exercise come from public comments in response to Public Citizens second petition for rulemaking to the Federal Election Commission (FEC) in July, 2023. Public Citizen asked the FEC to clarify the subject of \"fraudulent misrepresentation\" regarding the use of AI, including deep fake technology, and open a Notice of Availability that would allow public comment.\n\nFifty pdfs were downloaded from the FEC site. Each pdf contained comments in response to Public Citizens petition.\n\nThese fifty documents made up the corpus for the text analysis. The process is described below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load packages\nlibrary(pdftools)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing poppler version 23.04.0\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: NLP\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(topicmodels)\nlibrary(syuzhet)\nlibrary(tokenizers)\nlibrary(here)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nhere() starts at /Users/andrewruiz/MADA_course/andrew_ruiz-MADA-portfolio\n```\n\n\n:::\n:::\n\n\n# Process:\n## Locate and read the PDFs\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify the folder containing the PDFs\npdf_folder <- here(\"data-exercise\", \"data\", \"raw-data\")\n\n# Read all PDF files from the folder\nfile_list <- list.files(path = pdf_folder, pattern = \"*.pdf\", full.names = TRUE)\n```\n:::\n\n\n## Process files\nNow that the files are located and read, we will begin processing them for analysis\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract text from each PDF\ntext_data <- lapply(file_list, pdf_text)\n\n# Combine the text into one character vector, one element per PDF\ntext_data_combined <- sapply(text_data, paste, collapse = \" \")\n\n# Create a corpus from the combined text\ndocs <- Corpus(VectorSource(text_data_combined))\n```\n:::\n\n\n## Cleaning the text\nTo analyze the text we will need to clean in and prepare it for use. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_corpus_initial <- function(corpus) {\n  original_length <- length(corpus)\n  # convert text to lowercase\n  corpus <- tm_map(corpus, content_transformer(tolower))\n  # remove punctuation\n  corpus <- tm_map(corpus, removePunctuation)\n  # remove numbers\n  corpus <- tm_map(corpus, removeNumbers)\n  # remove stop words (common words like 'the', 'and', 'is)\n  corpus <- tm_map(corpus, removeWords, stopwords(\"english\"))\n  # remove extra whitespaces\n  corpus <- tm_map(corpus, stripWhitespace)\n  \n  # Return the cleaned corpus along with the indices of dropped documents\n  return(list(corpus = corpus, original_length = original_length))\n}\n\n# Apply initial cleaning\nresult <- clean_corpus_initial(docs)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in tm_map.SimpleCorpus(corpus, content_transformer(tolower)):\ntransformation drops documents\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in tm_map.SimpleCorpus(corpus, removePunctuation): transformation drops\ndocuments\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in tm_map.SimpleCorpus(corpus, removeNumbers): transformation drops\ndocuments\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in tm_map.SimpleCorpus(corpus, removeWords, stopwords(\"english\")):\ntransformation drops documents\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in tm_map.SimpleCorpus(corpus, stripWhitespace): transformation drops\ndocuments\n```\n\n\n:::\n\n```{.r .cell-code}\n# There was a warning that documents had been dropped. This code will check to see which ones and how many.\ncorpus_cleaned <- result$corpus\noriginal_length <- result$original_length\n\ncat(\"Original number of documents:\", original_length, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal number of documents: 50 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Number of documents after cleaning:\", length(corpus_cleaned), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of documents after cleaning: 50 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Number of documents dropped:\", original_length - length(corpus_cleaned), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of documents dropped: 0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Identify the indices of dropped documents\ndropped_indices <- setdiff(1:original_length, 1:length(corpus_cleaned))\ncat(\"Indices of dropped documents:\", paste(dropped_indices, collapse = \", \"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIndices of dropped documents:  \n```\n\n\n:::\n\n```{.r .cell-code}\n# The output indicates that zero documents were dropped\n```\n:::\n\n\nNow that the text is clean we can proceed to the next step.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a Document-Term Matrix (DTM) from the cleaned text documents (docs_cleaned_initial).\n# Remove rows (documents) from the DTM where the sum of term frequencies is zero,\n# effectively filtering out empty documents.\ndtm_initial <- DocumentTermMatrix(corpus_cleaned)\ndtm_initial <- dtm_initial[rowSums(as.matrix(dtm_initial)) > 0, ]\n```\n:::\n\nWe will now create a Latent Dirichlet Allocation (LDA) model. An LDA is statistical model used in natural language processing and machine learning.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the number of topics to 8\n# this can be modified depending on the need and results\nk_initial <- 8\n\n# Create an LDA model using the Document-Term Matrix (dtm_initial) with the specified number of topics.\n# Control parameters may include the random seed for reproducibility (seed = 4321).\nlda_model_initial <- LDA(dtm_initial, k = k_initial, control = list(seed = 4321))\n\n# Retrieve the terms associated with each topic, specifying a maximum of 8 terms per topic.\ntopics_initial <- terms(lda_model_initial, 8)\n\n# Print the initial topics along with potential terms that describe each topic.\nprint(\"Initial topics with potential names included:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Initial topics with potential names included:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(topics_initial)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Topic 1        Topic 2   Topic 3      Topic 4    Topic 5     Topic 6     \n[1,] \"campaign\"     \"mary\"    \"campaign\"   \"john\"     \"ads\"       \"content\"   \n[2,] \"comments\"     \"michael\" \"deepfakes\"  \"patricia\" \"provided\"  \"can\"       \n[3,] \"ads\"          \"robert\"  \"election\"   \"donna\"    \"campaigns\" \"campaign\"  \n[4,] \"deceptive\"    \"david\"   \"candidate\"  \"susan\"    \"never\"     \"generative\"\n[5,] \"content\"      \"barbara\" \"political\"  \"linda\"    \"depicting\" \"election\"  \n[6,] \"misleading\"   \"richard\" \"commission\" \"margaret\" \"law\"       \"federal\"   \n[7,] \"saying\"       \"linda\"   \"public\"     \"thomas\"   \"comments\"  \"use\"       \n[8,] \"ai‐generated\" \"susan\"   \"fraudulent\" \"david\"    \"use\"       \"commission\"\n     Topic 7      Topic 8    \n[1,] \"campaign\"   \"campaign\" \n[2,] \"comments\"   \"ads\"      \n[3,] \"saying\"     \"deceptive\"\n[4,] \"deceptive\"  \"law\"      \n[5,] \"misleading\" \"campaigns\"\n[6,] \"content\"    \"never\"    \n[7,] \"understand\" \"comments\" \n[8,] \"release\"    \"depicting\"\n```\n\n\n:::\n:::\n\nWe now have 8 topics with the most common words associated with each topic. Notice that topic 2 is a list of first names. This is not especially helpful in this case. However, let's proceed with these topics and see if we can fix them later.\nNext we will see the theme associated with each document. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the topic for each document for the initial model\ntopic_probabilities_initial <- posterior(lda_model_initial)$topics\ndoc_topics_initial <- apply(topic_probabilities_initial, 1, which.max)\n\n# Create a data frame for the document-topic associations for the initial model\ndoc_topics_df_initial <- data.frame(Document = names(doc_topics_initial), MostLikelyTopic = doc_topics_initial)\n\n# View the first few rows of the document-topic association for the initial model\nhead(doc_topics_df_initial)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Document MostLikelyTopic\n1        1               3\n2        2               3\n3        3               6\n4        4               6\n5        5               3\n6        6               6\n```\n\n\n:::\n:::\n\n\nLet's include those names in the stopword list to see if the results are better. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Extend the stopwords list with common names for refined cleaning\ncustom_stopwords <- c(stopwords(\"en\"), \"john\", \"patricia\", \"donna\", \"susan\", \"linda\", \"margaret\", \"thomas\", \"david\")\n\n# Refined cleaning function that includes removal of first names\nclean_corpus_refined <- function(corpus) {\n  corpus <- tm_map(corpus, content_transformer(tolower))\n  corpus <- tm_map(corpus, removePunctuation)\n  corpus <- tm_map(corpus, removeNumbers)\n  corpus <- tm_map(corpus, removeWords, custom_stopwords)\n  corpus <- tm_map(corpus, stripWhitespace)\n  return(corpus)\n}\n# Apply refined cleaning\ndocs_cleaned_refined <- clean_corpus_refined(docs)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in tm_map.SimpleCorpus(corpus, content_transformer(tolower)):\ntransformation drops documents\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in tm_map.SimpleCorpus(corpus, removePunctuation): transformation drops\ndocuments\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in tm_map.SimpleCorpus(corpus, removeNumbers): transformation drops\ndocuments\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in tm_map.SimpleCorpus(corpus, removeWords, custom_stopwords):\ntransformation drops documents\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in tm_map.SimpleCorpus(corpus, stripWhitespace): transformation drops\ndocuments\n```\n\n\n:::\n:::\n\nNow let's rerun the code with the refined text.\n\n::: {.cell}\n\n```{.r .cell-code}\n# DTM for refined analysis\ndtm_refined <- DocumentTermMatrix(docs_cleaned_refined)\ndtm_refined <- dtm_refined[rowSums(as.matrix(dtm_refined)) > 0, ]\n\n\n# Refined topic modeling\nk_refined <- 8\nlda_model_refined <- LDA(dtm_refined, k = k_refined, control = list(seed = 4321))\ntopics_refined <- terms(lda_model_refined, 8)\nprint(\"Refined topics without first names:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Refined topics without first names:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(topics_refined)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Topic 1      Topic 2      Topic 3      Topic 4   Topic 5     Topic 6    \n[1,] \"never\"      \"content\"    \"never\"      \"mary\"    \"ads\"       \"ads\"      \n[2,] \"ads\"        \"election\"   \"ads\"        \"michael\" \"campaign\"  \"campaign\" \n[3,] \"law\"        \"campaign\"   \"comments\"   \"barbara\" \"campaigns\" \"never\"    \n[4,] \"campaign\"   \"deepfakes\"  \"law\"        \"robert\"  \"deceptive\" \"comments\" \n[5,] \"misleading\" \"federal\"    \"campaign\"   \"nancy\"   \"provided\"  \"law\"      \n[6,] \"americans\"  \"use\"        \"provided\"   \"richard\" \"depicting\" \"provided\" \n[7,] \"content\"    \"generative\" \"use\"        \"james\"   \"comments\"  \"campaigns\"\n[8,] \"worried\"    \"elections\"  \"misleading\" \"carol\"   \"voters\"    \"depicting\"\n     Topic 7             Topic 8     \n[1,] \"campaign\"          \"campaign\"  \n[2,] \"candidate\"         \"ads\"       \n[3,] \"commission\"        \"fec\"       \n[4,] \"fraudulent\"        \"election\"  \n[5,] \"political\"         \"generative\"\n[6,] \"deepfakes\"         \"federal\"   \n[7,] \"public\"            \"can\"       \n[8,] \"misrepresentation\" \"political\" \n```\n\n\n:::\n:::\n\nSo it turns out that adding the names as stop words was not that helpful. It jsut returned another topic filled with names. For now, we will move on.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the topic for each document for the refined model\ntopic_probabilities_refined <- posterior(lda_model_refined)$topics\ndoc_topics_refined <- apply(topic_probabilities_refined, 1, which.max)\n\n# Create a data frame for the document-topic associations for the refined model\ndoc_topics_df_refined <- data.frame(Document = names(doc_topics_refined), MostLikelyTopic = doc_topics_refined)\n\n# View the first few rows of the document-topic association for the refined model\nhead(doc_topics_df_refined)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Document MostLikelyTopic\n1        1               2\n2        2               2\n3        3               2\n4        4               2\n5        5               2\n6        6               2\n```\n\n\n:::\n:::\n\nIt turns out that topic two is not associated with documents 1-6 now that names were added to the stopword list. More investigation would be needed to uncover the meaning of this.\n## Sentiment analysis\nnow let's take a look at the sentiment for each document. For this we will use the Bing method\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform sentiment analysis on the combined text data\nsentiment_scores <- get_sentiment(text_data_combined, method = \"bing\")\n\n# View the sentiment scores\nhead(sentiment_scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  -6 -24  -6  30  11  -8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create a vector of PDF document names\npdf_document_names <- basename(file_list)\n\n# Create a data frame with document names and sentiment scores\nsentiment_df <- data.frame(DocumentName = pdf_document_names, SentimentScore = sentiment_scores)\n\n# Print the first few rows of the data frame to see the mapping\nhead(sentiment_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          DocumentName SentimentScore\n1             aapc.pdf             -6\n2 accountable_tech.pdf            -24\n3              acm.pdf             -6\n4            Adobe.pdf             30\n5          afl_cio.pdf             11\n6         arnetfox.pdf             -8\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(sentiment_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                           DocumentName SentimentScore\n1                              aapc.pdf             -6\n2                  accountable_tech.pdf            -24\n3                               acm.pdf             -6\n4                             Adobe.pdf             30\n5                           afl_cio.pdf             11\n6                          arnetfox.pdf             -8\n7  AsianAmericans_advancing_justice.pdf            -16\n8                       brennan_ctr.pdf             -7\n9                                BS.pdf              2\n10                               bv.pdf             -2\n11                               ca.pdf            -72\n12      campaign_for_accountability.pdf            -13\n13             catholic_social_just.pdf              1\n14                               cb.pdf           -131\n15                     common_cause.pdf            -37\n16                              CPD.pdf             -5\n17                             CREW.pdf             -6\n18                            crew2.pdf             -4\n19               ctr_democracy_tech.pdf             -5\n20            ctr_for_ai_dig_policy.pdf              0\n21                               dc.pdf           -222\n22                       demo_first.pdf             -3\n23                              dnc.pdf            -13\n24              election_protection.pdf             -2\n25                             epic.pdf              4\n26                     future_priva.pdf             -3\n27                              GMU.pdf             -4\n28                       harvardlaw.pdf            -20\n29                         holtzman.pdf            -13\n30 Institute for Strategic Dialogue.pdf             -5\n31                   integrity_inst.pdf              1\n32                        issue_dia.pdf              2\n33                               jm.pdf              3\n34                               lc.pdf           -241\n35                              lwv.pdf              2\n36                               MM.pdf              4\n37                               MR.pdf            -24\n38                        partnerAI.pdf              3\n39              people_power_united.pdf             -4\n40                              ppu.pdf              5\n41                protect_democracy.pdf            -13\n42                         pub_citz.pdf             -1\n43                     she_persists.pdf            -21\n44                      stabilityAI.pdf             21\n45                        StanfordU.pdf              3\n46                          technet.pdf             10\n47                           unidos.pdf            -21\n48                      US_congress.pdf             -1\n49                            wiley.pdf            -28\n50                   workers_circle.pdf             -2\n```\n\n\n:::\n:::\n\nFor the text used in this analysis, the sentiment may be a little misleading. These comments were written in support of a second petition for the FEC to allow public comments of rulemaking. Most public comments in these forums begin by thanking the regulatory agency for allowing comments. Those sections tend to be very positive. \nHowever, the comments often continue by describing potential problem. Those tend to use negative language.\nThe Bing method results are centered around 0. A zero score indicates completely neutral setiment. The larger negative scores indicate negative sentiment. Large positive scores indicate positive sentiment. \n\n### Let's take a look at a different way to identify sentiment. For this we will use the NRC method which classifies sentiment into categories that may make better sense of the data.\nThese results display word counts for the number of words in each document that fall into NRC's sentiment categories. \nThe bar graph represents the percent to words that fall into each category. The barchart represents all the text across all 50 documents. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define the data\nnrc_data <- get_nrc_sentiment(text_data_combined)\n\n# Access the data frame columns for emotions and sentiments\n#anger_items <- which(nrc_data$anger > 0)\n#joy_items <- which(nrc_data$joy > 0)\n\n# Print sentences associated with specific emotions\n#print(text_data_combined[anger_items])\n#print(text_data_combined[joy_items])\n\n# View the entire sentiment data frame\nprint(nrc_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   anger anticipation disgust fear joy sadness surprise trust negative positive\n1     10            6       6   13   5       3        4    22       21       37\n2     21           13      11   22   7       6        7    28       42       24\n3     11           15       7   15   8       7        5    34       26       44\n4     14           24       7   20  20      13        9    51       30       92\n5     16           16       7   19  12      12        7    37       29       56\n6     11           11       4   12   8       4        2    27       19       43\n7     17           11       6   24   6      13        4    28       34       44\n8     18           18       8   16   9       7        3    34       30       50\n9      1            4       1    2   2       1        1    11        8       20\n10     2            0       1    3   2       0        0     2        2        4\n11    68           48      39   75  29      52       27    97      133      158\n12    26           19      10   25  13      17        5    53       47       72\n13     6            8       3   11   7       2        5    17       15       24\n14   116           86      77  134  62      97       46   163      251      263\n15    31           21      15   38  15      26        9    58       66       92\n16     7            8       3   13   6       3        4    23       15       30\n17     9            3       6    7   3       4        2    11       16       18\n18     6            5       4    9   3       2        3    19       13       23\n19    18           17      10   21  13      15        7    48       46       79\n20    26           42      16   37  21      22       18    89       62      132\n21   161          139     100  194  93     131       79   245      374      390\n22     2            4       2    3   4       3        2    10       11       19\n23    17            8      12   18   5      10        7    34       35       47\n24     2            2       0    4   2       0        1     3        7       10\n25    11           10       5   13  14       9        6    32       30       69\n26    23           20      10   23  14      12        8    48       39       80\n27    19           23       8   21  22      13       10    58       48       83\n28    34           33      17   36  17      18       11    61       74       94\n29    20           18      13   19  10      13        7    43       51       59\n30     4            2       2    3   3       4        0     9        9       15\n31    24           35      11   35  19      16       11    78       57      124\n32     9           11       5   15  11      12        7    40       25       51\n33     2            1       2    3   5       2        0    11        7       16\n34   188          124     130  213  83     168       81   229      414      357\n35    12           15       5   11  10       6        5    29       20       52\n36     1            1       0    1   1       0        0     4        1        4\n37    17            9       8   23   7      10        2    38       44       53\n38    14           22       8   16  18      10        9    53       33       74\n39     3            3       2    4   3       3        1    11        8       19\n40    28           30      20   28  36      21       14    46       54       96\n41    21           15      12   20  12      17        6    52       49       72\n42    20           21      14   18  20      19       12    65       59       91\n43    16           14       8   21  12       6        5    34       35       48\n44    16           27       9   21  20      10        6    56       37       92\n45    10           10       6   12   6       5        0    28       22       44\n46     6            9       3    8   6       3        3    25       15       42\n47    19           16       8   20   7      12        5    41       35       61\n48     2            2       2    5   3       1        2    13        5       23\n49    15           13       8   22  13       9       11    45       44       56\n50     5            3       3    4   3       3        1    10       10       17\n```\n\n\n:::\n\n```{.r .cell-code}\n# View only the positive and negative valence columns\nprint(nrc_data[, c(\"negative\", \"positive\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   negative positive\n1        21       37\n2        42       24\n3        26       44\n4        30       92\n5        29       56\n6        19       43\n7        34       44\n8        30       50\n9         8       20\n10        2        4\n11      133      158\n12       47       72\n13       15       24\n14      251      263\n15       66       92\n16       15       30\n17       16       18\n18       13       23\n19       46       79\n20       62      132\n21      374      390\n22       11       19\n23       35       47\n24        7       10\n25       30       69\n26       39       80\n27       48       83\n28       74       94\n29       51       59\n30        9       15\n31       57      124\n32       25       51\n33        7       16\n34      414      357\n35       20       52\n36        1        4\n37       44       53\n38       33       74\n39        8       19\n40       54       96\n41       49       72\n42       59       91\n43       35       48\n44       37       92\n45       22       44\n46       15       42\n47       35       61\n48        5       23\n49       44       56\n50       10       17\n```\n\n\n:::\n\n```{.r .cell-code}\ndocument_sentiment <- data.frame(DocumentName = pdf_document_names, \n                                 Negative = nrc_data$negative, \n                                 Positive = nrc_data$positive)\n\n# Print the data frame with document names and sentiment scores\nprint(document_sentiment)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                           DocumentName Negative Positive\n1                              aapc.pdf       21       37\n2                  accountable_tech.pdf       42       24\n3                               acm.pdf       26       44\n4                             Adobe.pdf       30       92\n5                           afl_cio.pdf       29       56\n6                          arnetfox.pdf       19       43\n7  AsianAmericans_advancing_justice.pdf       34       44\n8                       brennan_ctr.pdf       30       50\n9                                BS.pdf        8       20\n10                               bv.pdf        2        4\n11                               ca.pdf      133      158\n12      campaign_for_accountability.pdf       47       72\n13             catholic_social_just.pdf       15       24\n14                               cb.pdf      251      263\n15                     common_cause.pdf       66       92\n16                              CPD.pdf       15       30\n17                             CREW.pdf       16       18\n18                            crew2.pdf       13       23\n19               ctr_democracy_tech.pdf       46       79\n20            ctr_for_ai_dig_policy.pdf       62      132\n21                               dc.pdf      374      390\n22                       demo_first.pdf       11       19\n23                              dnc.pdf       35       47\n24              election_protection.pdf        7       10\n25                             epic.pdf       30       69\n26                     future_priva.pdf       39       80\n27                              GMU.pdf       48       83\n28                       harvardlaw.pdf       74       94\n29                         holtzman.pdf       51       59\n30 Institute for Strategic Dialogue.pdf        9       15\n31                   integrity_inst.pdf       57      124\n32                        issue_dia.pdf       25       51\n33                               jm.pdf        7       16\n34                               lc.pdf      414      357\n35                              lwv.pdf       20       52\n36                               MM.pdf        1        4\n37                               MR.pdf       44       53\n38                        partnerAI.pdf       33       74\n39              people_power_united.pdf        8       19\n40                              ppu.pdf       54       96\n41                protect_democracy.pdf       49       72\n42                         pub_citz.pdf       59       91\n43                     she_persists.pdf       35       48\n44                      stabilityAI.pdf       37       92\n45                        StanfordU.pdf       22       44\n46                          technet.pdf       15       42\n47                           unidos.pdf       35       61\n48                      US_congress.pdf        5       23\n49                            wiley.pdf       44       56\n50                   workers_circle.pdf       10       17\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create a bar graph of emotions\nbarplot(\n  sort(colSums(prop.table(nrc_data[, 1:8]))), \n  horiz = TRUE, \n  cex.names = 0.7, \n  las = 1, \n  main = \"Emotions in Text\", \n  xlab = \"Percentage\"\n)\n```\n\n::: {.cell-output-display}\n![](data-exercise2_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "data-exercise2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}