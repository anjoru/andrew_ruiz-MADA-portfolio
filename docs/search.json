[
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html",
    "href": "ml-models-exercise/ml-models-exercise.html",
    "title": "ml-models-exercise",
    "section": "",
    "text": "library(here)\n\nhere() starts at /Users/andrewruiz/andrew_ruiz-MADA-portfolio\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5      ✔ recipes      1.0.10\n✔ dials        1.2.1      ✔ rsample      1.2.0 \n✔ dplyr        1.1.4      ✔ tibble       3.2.1 \n✔ ggplot2      3.5.0      ✔ tidyr        1.3.1 \n✔ infer        1.0.6      ✔ tune         1.1.2 \n✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.0      ✔ workflowsets 1.0.1 \n✔ purrr        1.0.2      ✔ yardstick    1.3.1 \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(ggplot2)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(ggcorrplot)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\nlibrary(ranger)\nlibrary(dplyr)\nlibrary(glmnet)\nlibrary(workflows)\nlibrary(yardstick)\n\n\n\n\n\n#Set the random seed to 1234\nex11_seed = 1234\nset.seed(ex11_seed)\n\n\n\n\n\n# Construct the path to the RDS file using here()\nfile_path_mav &lt;- here(\"ml-models-exercise\", \"mav_clean_ex11.rds\")\n\n# Load the mav_clean dataframe from the specified RDS file\nmav_ex11 &lt;- readRDS(file_path_mav)\n\n# examine the dataset\nhead(mav_ex11)\n\n# A tibble: 6 × 7\n      Y DOSE    AGE SEX   RACE     WT    HT\n  &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2691. 25       42 1     2      94.3  1.77\n2 2639. 25       24 1     2      80.4  1.76\n3 2150. 25       31 1     1      71.8  1.81\n4 1789. 25       46 2     1      77.4  1.65\n5 3126. 25       41 2     2      64.3  1.56\n6 2337. 25       27 1     2      74.1  1.83\n\ndim(mav_ex11)\n\n[1] 120   7\n\n\n\n\n\n\n# Count occurrences of each category in the RACE variable\nrace_counts &lt;- table(mav_ex11$RACE)\n\n# Calculate percentages\nrace_percentages &lt;- (race_counts / sum(race_counts)) * 100\n\n# Combine counts and percentages into a data frame for better readability\nrace_summary &lt;- data.frame(\n  Race = names(race_counts),\n  Counts = as.integer(race_counts), # Ensure counts are in integer format\n  Percentages = race_percentages\n)\n# Print the summary data frame\nprint(race_summary)\n\n  Race Counts Percentages.Var1 Percentages.Freq\n1    1     74                1        61.666667\n2    2     36                2        30.000000\n3    7      2                7         1.666667\n4   88      8               88         6.666667\n\n\n\n\n\n\n\nModel-Based Evaluation of the Impact of Formulation and Food Intake on the Complex Oral Absorption of Mavoglurant in Healthy Subjects. Pharm Res 32, 1764–1778 (2015). https://doi.org/10.1007/s11095-014-1574-1 race break down was: Caucasian (61.7), Black (30), Native American (1.7) and Other (6.7)\n\n\n\n1=Caucasian 2=Black 7=Native American 88=Other\n\n\n\n\n# Recode RACE\nmav_ex11 &lt;- mav_ex11 %&gt;%\n  mutate(RACE = as.character(RACE)) %&gt;% # Convert to character to handle NA properly\n  mutate(RACE = recode(RACE, '7' = '3', '88' = '3')) %&gt;%\n  mutate(RACE = factor(RACE, levels = c('1', '2', '3'), \n                       labels = c(\"Caucasian\", \"Black\", \"Other\")))\n\n# Now, print the levels of RACE after recoding:\nprint(levels(mav_ex11$RACE))\n\n[1] \"Caucasian\" \"Black\"     \"Other\"    \n\n\n\n\n\n\n#####If we were to find any very strong correlations, we might want to remove those.\n\n# Selecting only continuous variables\ncontinuous_vars &lt;- mav_ex11[, c(\"AGE\", \"WT\", \"HT\")]\n\n# Computing the correlation matrix\ncor_matrix &lt;- cor(continuous_vars, use = \"complete.obs\")\n\n# Visualizing the correlation matrix\nggcorrplot(cor_matrix, method = \"circle\", hc.order = TRUE, type = \"lower\",\n           lab = TRUE, lab_size = 3, colors = c(\"gold\", \"snow\", \"tomato\"))\n\n\n\n\n\n\n\nprint(cor_matrix)\n\n           AGE        WT         HT\nAGE  1.0000000 0.1196740 -0.3518581\nWT   0.1196740 1.0000000  0.5997505\nHT  -0.3518581 0.5997505  1.0000000\n\n\n\n\n\n\n\n\n# Calculate BMI\nmav_ex11$BMI &lt;- mav_ex11$WT / (mav_ex11$HT^2)\n\nstr(mav_ex11)\n\ntibble [120 × 8] (S3: tbl_df/tbl/data.frame)\n $ Y   : num [1:120] 2691 2639 2150 1789 3126 ...\n $ DOSE: Factor w/ 3 levels \"25\",\"37.5\",\"50\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AGE : int [1:120] 42 24 31 46 41 27 23 20 23 28 ...\n $ SEX : Factor w/ 2 levels \"1\",\"2\": 1 1 1 2 2 1 1 1 1 1 ...\n $ RACE: Factor w/ 3 levels \"Caucasian\",\"Black\",..: 2 2 1 1 2 2 1 3 2 1 ...\n $ WT  : num [1:120] 94.3 80.4 71.8 77.4 64.3 ...\n $ HT  : num [1:120] 1.77 1.76 1.81 1.65 1.56 ...\n $ BMI : num [1:120] 30.1 26 21.9 28.4 26.4 ...\n\nmav_ex11 &lt;- mav_ex11 %&gt;%\n  mutate(\n    DOSE = as.factor(DOSE),\n    RACE = as.factor(RACE),\n    SEX = as.factor(SEX)\n  )\n\n\n\n\n\n\n# Define a linear regression model\nlinear_model_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# Prepare the recipe for the linear model, specifying categorical variables\nlinear_model_recipe &lt;- recipe(Y ~ ., data = mav_ex11) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) # Convert categorical variables to dummy variables\n\n# Combine the model and recipe into a workflow, then fit it to the data\nlinear_model_workflow &lt;- workflow() %&gt;%\n  add_model(linear_model_spec) %&gt;%\n  add_recipe(linear_model_recipe) %&gt;%\n  fit(data = mav_ex11)\n\n\n\n\n\n# Then define the recipe for LASSO\nlasso_model_recipe &lt;- recipe(Y ~ ., data = mav_ex11) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\nlasso_model_spec &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Recipe for LASSO, converting categorical variables to dummy variables\nlasso_model_recipe &lt;- recipe(Y ~ ., data = mav_ex11) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n  \n\n# Workflow for LASSO, combining model and recipe, then fitting to data\nlasso_model_workflow &lt;- workflow() %&gt;%\n  add_model(lasso_model_spec) %&gt;%\n  add_recipe(lasso_model_recipe) %&gt;%\n  fit(data = mav_ex11)\n\n\n\n\n\n# Random Forest Model Specification with ranger engine\nrf_model_spec &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", seed = ex11_seed) %&gt;%\n  set_mode(\"regression\")\n\n# Recipe for Random Forest, ensuring categorical variables are treated correctly\n# No need for step_dummy() as random forest can handle categorical variables directly\nrf_model_recipe &lt;- recipe(Y ~ ., data = mav_ex11)\n\n# Workflow for Random Forest, combining model and recipe, then fitting to data\nrf_model_workflow &lt;- workflow() %&gt;%\n  add_model(rf_model_spec) %&gt;%\n  add_recipe(rf_model_recipe) %&gt;%\n  fit(data = mav_ex11)"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#load-libraries",
    "href": "ml-models-exercise/ml-models-exercise.html#load-libraries",
    "title": "ml-models-exercise",
    "section": "",
    "text": "library(here)\n\nhere() starts at /Users/andrewruiz/andrew_ruiz-MADA-portfolio\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5      ✔ recipes      1.0.10\n✔ dials        1.2.1      ✔ rsample      1.2.0 \n✔ dplyr        1.1.4      ✔ tibble       3.2.1 \n✔ ggplot2      3.5.0      ✔ tidyr        1.3.1 \n✔ infer        1.0.6      ✔ tune         1.1.2 \n✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.0      ✔ workflowsets 1.0.1 \n✔ purrr        1.0.2      ✔ yardstick    1.3.1 \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(ggplot2)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(ggcorrplot)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\nlibrary(ranger)\nlibrary(dplyr)\nlibrary(glmnet)\nlibrary(workflows)\nlibrary(yardstick)"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#set-the-seed-for-reproducibility",
    "href": "ml-models-exercise/ml-models-exercise.html#set-the-seed-for-reproducibility",
    "title": "ml-models-exercise",
    "section": "",
    "text": "#Set the random seed to 1234\nex11_seed = 1234\nset.seed(ex11_seed)"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#load-rds-from-the-fitting-exercise",
    "href": "ml-models-exercise/ml-models-exercise.html#load-rds-from-the-fitting-exercise",
    "title": "ml-models-exercise",
    "section": "",
    "text": "# Construct the path to the RDS file using here()\nfile_path_mav &lt;- here(\"ml-models-exercise\", \"mav_clean_ex11.rds\")\n\n# Load the mav_clean dataframe from the specified RDS file\nmav_ex11 &lt;- readRDS(file_path_mav)\n\n# examine the dataset\nhead(mav_ex11)\n\n# A tibble: 6 × 7\n      Y DOSE    AGE SEX   RACE     WT    HT\n  &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2691. 25       42 1     2      94.3  1.77\n2 2639. 25       24 1     2      80.4  1.76\n3 2150. 25       31 1     1      71.8  1.81\n4 1789. 25       46 2     1      77.4  1.65\n5 3126. 25       41 2     2      64.3  1.56\n6 2337. 25       27 1     2      74.1  1.83\n\ndim(mav_ex11)\n\n[1] 120   7"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#examine-the-race-variable",
    "href": "ml-models-exercise/ml-models-exercise.html#examine-the-race-variable",
    "title": "ml-models-exercise",
    "section": "",
    "text": "# Count occurrences of each category in the RACE variable\nrace_counts &lt;- table(mav_ex11$RACE)\n\n# Calculate percentages\nrace_percentages &lt;- (race_counts / sum(race_counts)) * 100\n\n# Combine counts and percentages into a data frame for better readability\nrace_summary &lt;- data.frame(\n  Race = names(race_counts),\n  Counts = as.integer(race_counts), # Ensure counts are in integer format\n  Percentages = race_percentages\n)\n# Print the summary data frame\nprint(race_summary)\n\n  Race Counts Percentages.Var1 Percentages.Freq\n1    1     74                1        61.666667\n2    2     36                2        30.000000\n3    7      2                7         1.666667\n4   88      8               88         6.666667"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#recode-the-race-variable",
    "href": "ml-models-exercise/ml-models-exercise.html#recode-the-race-variable",
    "title": "ml-models-exercise",
    "section": "",
    "text": "Model-Based Evaluation of the Impact of Formulation and Food Intake on the Complex Oral Absorption of Mavoglurant in Healthy Subjects. Pharm Res 32, 1764–1778 (2015). https://doi.org/10.1007/s11095-014-1574-1 race break down was: Caucasian (61.7), Black (30), Native American (1.7) and Other (6.7)\n\n\n\n1=Caucasian 2=Black 7=Native American 88=Other\n\n\n\n\n# Recode RACE\nmav_ex11 &lt;- mav_ex11 %&gt;%\n  mutate(RACE = as.character(RACE)) %&gt;% # Convert to character to handle NA properly\n  mutate(RACE = recode(RACE, '7' = '3', '88' = '3')) %&gt;%\n  mutate(RACE = factor(RACE, levels = c('1', '2', '3'), \n                       labels = c(\"Caucasian\", \"Black\", \"Other\")))\n\n# Now, print the levels of RACE after recoding:\nprint(levels(mav_ex11$RACE))\n\n[1] \"Caucasian\" \"Black\"     \"Other\""
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#make-a-pairwise-correlation-plot-for-the-continuous-variables.",
    "href": "ml-models-exercise/ml-models-exercise.html#make-a-pairwise-correlation-plot-for-the-continuous-variables.",
    "title": "ml-models-exercise",
    "section": "",
    "text": "#####If we were to find any very strong correlations, we might want to remove those.\n\n# Selecting only continuous variables\ncontinuous_vars &lt;- mav_ex11[, c(\"AGE\", \"WT\", \"HT\")]\n\n# Computing the correlation matrix\ncor_matrix &lt;- cor(continuous_vars, use = \"complete.obs\")\n\n# Visualizing the correlation matrix\nggcorrplot(cor_matrix, method = \"circle\", hc.order = TRUE, type = \"lower\",\n           lab = TRUE, lab_size = 3, colors = c(\"gold\", \"snow\", \"tomato\"))\n\n\n\n\n\n\n\nprint(cor_matrix)\n\n           AGE        WT         HT\nAGE  1.0000000 0.1196740 -0.3518581\nWT   0.1196740 1.0000000  0.5997505\nHT  -0.3518581 0.5997505  1.0000000"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#while-the-correlation-between-ht-and-wt-is-not-strong-we-will-combine-the-two-into-bmi-calculating-new-variable-bmi-from-htm-and-wtkg.",
    "href": "ml-models-exercise/ml-models-exercise.html#while-the-correlation-between-ht-and-wt-is-not-strong-we-will-combine-the-two-into-bmi-calculating-new-variable-bmi-from-htm-and-wtkg.",
    "title": "ml-models-exercise",
    "section": "",
    "text": "# Calculate BMI\nmav_ex11$BMI &lt;- mav_ex11$WT / (mav_ex11$HT^2)\n\nstr(mav_ex11)\n\ntibble [120 × 8] (S3: tbl_df/tbl/data.frame)\n $ Y   : num [1:120] 2691 2639 2150 1789 3126 ...\n $ DOSE: Factor w/ 3 levels \"25\",\"37.5\",\"50\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AGE : int [1:120] 42 24 31 46 41 27 23 20 23 28 ...\n $ SEX : Factor w/ 2 levels \"1\",\"2\": 1 1 1 2 2 1 1 1 1 1 ...\n $ RACE: Factor w/ 3 levels \"Caucasian\",\"Black\",..: 2 2 1 1 2 2 1 3 2 1 ...\n $ WT  : num [1:120] 94.3 80.4 71.8 77.4 64.3 ...\n $ HT  : num [1:120] 1.77 1.76 1.81 1.65 1.56 ...\n $ BMI : num [1:120] 30.1 26 21.9 28.4 26.4 ...\n\nmav_ex11 &lt;- mav_ex11 %&gt;%\n  mutate(\n    DOSE = as.factor(DOSE),\n    RACE = as.factor(RACE),\n    SEX = as.factor(SEX)\n  )"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#create-linear-model-with-all-predictors.",
    "href": "ml-models-exercise/ml-models-exercise.html#create-linear-model-with-all-predictors.",
    "title": "ml-models-exercise",
    "section": "",
    "text": "# Define a linear regression model\nlinear_model_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# Prepare the recipe for the linear model, specifying categorical variables\nlinear_model_recipe &lt;- recipe(Y ~ ., data = mav_ex11) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) # Convert categorical variables to dummy variables\n\n# Combine the model and recipe into a workflow, then fit it to the data\nlinear_model_workflow &lt;- workflow() %&gt;%\n  add_model(linear_model_spec) %&gt;%\n  add_recipe(linear_model_recipe) %&gt;%\n  fit(data = mav_ex11)"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#lasso-regression-model-specification-with-glmnet-engine",
    "href": "ml-models-exercise/ml-models-exercise.html#lasso-regression-model-specification-with-glmnet-engine",
    "title": "ml-models-exercise",
    "section": "",
    "text": "# Then define the recipe for LASSO\nlasso_model_recipe &lt;- recipe(Y ~ ., data = mav_ex11) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\nlasso_model_spec &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Recipe for LASSO, converting categorical variables to dummy variables\nlasso_model_recipe &lt;- recipe(Y ~ ., data = mav_ex11) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n  \n\n# Workflow for LASSO, combining model and recipe, then fitting to data\nlasso_model_workflow &lt;- workflow() %&gt;%\n  add_model(lasso_model_spec) %&gt;%\n  add_recipe(lasso_model_recipe) %&gt;%\n  fit(data = mav_ex11)"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#random-forest-model-specification-with-ranger-engine",
    "href": "ml-models-exercise/ml-models-exercise.html#random-forest-model-specification-with-ranger-engine",
    "title": "ml-models-exercise",
    "section": "",
    "text": "# Random Forest Model Specification with ranger engine\nrf_model_spec &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", seed = ex11_seed) %&gt;%\n  set_mode(\"regression\")\n\n# Recipe for Random Forest, ensuring categorical variables are treated correctly\n# No need for step_dummy() as random forest can handle categorical variables directly\nrf_model_recipe &lt;- recipe(Y ~ ., data = mav_ex11)\n\n# Workflow for Random Forest, combining model and recipe, then fitting to data\nrf_model_workflow &lt;- workflow() %&gt;%\n  add_model(rf_model_spec) %&gt;%\n  add_recipe(rf_model_recipe) %&gt;%\n  fit(data = mav_ex11)"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#examine-the-rsme-values-from-the-models",
    "href": "ml-models-exercise/ml-models-exercise.html#examine-the-rsme-values-from-the-models",
    "title": "ml-models-exercise",
    "section": "Examine the RSME values from the models",
    "text": "Examine the RSME values from the models\n\n#RMSE\n# Create a dataframe to hold the RMSE values\nrmse_summary_ex11 &lt;- tibble(\n  Model = c(\"Linear\", \"LASSO\", \"Random Forest\"),\n  RMSE = c(lm_rmse_ex11$.estimate, lasso_rmse_ex11$.estimate, rf_rmse_ex11$.estimate)\n)\n\n# Print the summary table\nprint(rmse_summary_ex11)\n\n# A tibble: 3 × 2\n  Model          RMSE\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Linear         570.\n2 LASSO          571.\n3 Random Forest  358."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#tuning-the-lasso-model-wihtout-cv",
    "href": "ml-models-exercise/ml-models-exercise.html#tuning-the-lasso-model-wihtout-cv",
    "title": "ml-models-exercise",
    "section": "Tuning the LASSO model wihtout CV",
    "text": "Tuning the LASSO model wihtout CV\n\nthis is not a good idea\n\n# Define the range of penalty values\npenalty_grid &lt;- 10^seq(-5, 2, length.out = 50)\n\n# Update the LASSO model spec to use tune() for the penalty\nlasso_model_spec_tuned &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Use the same recipe as before\nlasso_model_recipe &lt;- recipe(Y ~ ., data = mav_ex11) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# Define the tuning workflow\nlasso_tuning_workflow &lt;- workflow() %&gt;%\n  add_model(lasso_model_spec_tuned) %&gt;%\n  add_recipe(lasso_model_recipe)\n\n# Create a tibble with penalty values for tuning\npenalty_tibble &lt;- tibble(penalty = penalty_grid)\n\n# Use apparent resampling for tuning (not recommended in practice)\nlasso_resamples &lt;- apparent(mav_ex11)\n\n# Tune the model\nlasso_tuned_results &lt;- tune_grid(\n  lasso_tuning_workflow,\n  resamples = lasso_resamples,\n  grid = penalty_tibble\n)\n\n# Evaluate the tuned model\nbest_lasso &lt;- select_best(lasso_tuned_results, \"rmse\")\n\n# Print the best penalty value\nprint(best_lasso)\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1 0.00001 Preprocessor1_Model01\n\n# Visualize tuning diagnostics for LASSO\nlasso_tuned_results %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nAs the penalty parameter increases, LASSO regression can drive more coefficients to zero, effectively removing them from the model. If the penalty is too large, it may remove too many features, leading the model towards a null model, which is a model with no predictors.\n\n\nThe unpenalized linear model is fully optimized to fit the training data without any restraint, possibly capturing noise and overfitting. When LASSO introduces a penalty for complexity, it trades off some of the training data fit to achieve a model that should generalize better. However, since we are only evaluating on the same data used to tune the penalty, we don’t see the benefit of this trade-off. In fact, due to this evaluation approach, we may see an increase in RMSE as we overly simplify the model, potentially leading to underfitting when the penalty is too high"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#now-for-randome-forest-model",
    "href": "ml-models-exercise/ml-models-exercise.html#now-for-randome-forest-model",
    "title": "ml-models-exercise",
    "section": "Now for Randome Forest model",
    "text": "Now for Randome Forest model\n\n# Define the Random Forest model with specific tuning indications\nrf_model_spec &lt;- rand_forest(trees = 300, mtry = tune(), min_n = tune()) %&gt;%\n  set_engine(\"ranger\", seed = 123) %&gt;%\n  set_mode(\"regression\")\n\n# Define the recipe\nrf_recipe &lt;- recipe(Y ~ ., data = mav_ex11)\n\n# Define the tuning grid\ntuning_grid &lt;- grid_regular(\n  mtry(range = c(1, 7)),\n  min_n(range = c(1, 21)),\n  levels = 7\n)\n\n# Define resampling method - using the full dataset through apparent resampling\nresamples &lt;- apparent(data = mav_ex11)\n\n# Combine the model and recipe into a workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_model_spec) %&gt;%\n  add_recipe(rf_recipe)\n\n# Tune the model using the workflow\nrf_tuned_results &lt;- tune_grid(\n  rf_workflow,\n  resamples = resamples,\n  grid = tuning_grid\n)\n\n# Visualize the tuning results\nautoplot(rf_tuned_results)"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#now-we-will-try-it-all-with-cv",
    "href": "ml-models-exercise/ml-models-exercise.html#now-we-will-try-it-all-with-cv",
    "title": "ml-models-exercise",
    "section": "Now we will try it all with CV",
    "text": "Now we will try it all with CV\n\nLASSO\n\n# Set the seed for reproducibility\nset.seed(ex11_seed)\n\n# Define the range of penalty values for the grid\npenalty_grid &lt;- 10^seq(-5, 2, length.out = 50)\n\n# Update the LASSO model spec to use tune() for the penalty\nlasso_model_spec_tuned &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Define the recipe, including dummy variables as needed\nlasso_model_recipe &lt;- recipe(Y ~ ., data = mav_ex11) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# Define a workflow that includes the model spec and the recipe\nlasso_tuning_workflow &lt;- workflow() %&gt;%\n  add_model(lasso_model_spec_tuned) %&gt;%\n  add_recipe(lasso_model_recipe)\n\n# Create the cross-validation resamples\ncv_resamples &lt;- vfold_cv(mav_ex11, v = 5, repeats = 5)\n\n# Tune the model with cross-validation\nlasso_tuned_cv_results &lt;- tune_grid(\n  lasso_tuning_workflow,\n  resamples = cv_resamples,\n  grid = penalty_tibble\n)\n\n# Visualize tuning diagnostics\nautoplot(lasso_tuned_cv_results)"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#random-forest",
    "href": "ml-models-exercise/ml-models-exercise.html#random-forest",
    "title": "ml-models-exercise",
    "section": "Random Forest",
    "text": "Random Forest\n\nrf_model_spec &lt;- rand_forest(trees = 300, mtry = tune(), min_n = tune()) %&gt;%\n  set_engine(\"ranger\", seed = ex11_seed) %&gt;%\n  set_mode(\"regression\")\n\n# Define the recipe\nrf_recipe &lt;- recipe(Y ~ ., data = mav_ex11)\n\n# Define the tuning grid\ntuning_grid &lt;- grid_regular(\n  mtry(range = c(1, 7)),\n  min_n(range = c(1, 21)),\n  levels = 7\n)\n\n# Define resampling method - using the full dataset through apparent resampling\nrf_cv_resamples &lt;- vfold_cv(data = mav_ex11)\n\n# Combine the model and recipe into a workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_model_spec) %&gt;%\n  add_recipe(rf_recipe)\n\n# Tune the model using the workflow\nrf_cvtuned_results &lt;- tune_grid(\n  rf_workflow,\n  resamples = resamples,\n  grid = tuning_grid\n)\n\n# Set the seed for reproducibility\nset.seed(ex11_seed)\n\n# Create 5-fold CV resamples, repeated 5 times\nrf_cv_resamples &lt;- vfold_cv(data = mav_ex11, v = 5, repeats = 5)\n\n# Tune the model using the workflow with the corrected resamples variable\nrf_cvtuned_results &lt;- tune_grid(\n  rf_workflow,\n  resamples = rf_cv_resamples,\n  grid = tuning_grid\n)\n\n# Visualize the tuning results\nautoplot(rf_cvtuned_results)\n\n\n\n\n\n\n\n\n\nWithout external validation data, it’s hard to fully assess which model generalizes better. Lower training error (like RMSE or MSE) doesn’t always guarantee better performance on unseen data. Models like RF can sometimes be more robust to overfitting compared to linear models, depending on the data and how the tuning is done.\n\n\nConclusion:\n\nBased on the given tuning results, the LASSO model appears to offer better predictive accuracy with a lower RMSE compared to the RF model’s MSE/RMSE. However, the final decision on which model to use should also consider factors such as model interpretability, computational cost, and how well you expect the model to generalize to new, unseen data."
  }
]